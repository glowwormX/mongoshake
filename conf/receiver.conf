# global log level. lower level message
# will be filter
log_level = debug
# log file name. use logs/ prefix folder path
log_file = reeciver.log
# log buffer or unbuffer. If set true, logs may not be print when exit. If
# set false, performance will be decreased
log_buffer = true


# profiling on net/http/profile
system_profile = 9501


# tunnel pipeline type. now we support rpc,tcp,file,mock,kafka
tunnel = file
# tunnel target resource url
# for rpc. this is receiver socket address
# for tcp. this is receiver socket address
# for file. this is the file path, for instance "data"
# for mock. this is useless. mongoshake will generate random data including "i", "d", "u", "n"
# for kafka. this is the topic and brokers address which split by comma, for
# instance: topic@brokers1,brokers2, default topic is "mongoshake"
tunnel.address = syncReceiver


# replayer worker concurrency. must equal to the collector worker number
replayer = 8

# ----------------------splitter----------------------
tunnel.writer = direct
tunnel.writer.address = mongodb://192.168.1.77:3001
tunnel.writer.file.read.time = 5
# if tunnel type is direct, all the below variable should be set

# only transfer oplog commands for syncing. represent
# by oplog.op are "i","d","u". also include applyOps
replayer.dml_only = true

# executors in single worker
replayer.executor = 1

# oplog changes to Insert while Update found non-exist (_id or unique-index)
replayer.executor.upsert = true
# oplog changes to Update while Insert found duplicated key (_id or unique-index)
replayer.executor.insert_on_dup_update = true
# db. write duplicated logs to mongoshake_conflict
# sdk. write duplicated logs to sdk.
replayer.conflict_write_to = none

# replayer duration mode. drop oplogs and take
# no any action(only for debugging enviroment) if
# set to false. otherwise write to ${mongo_url} instance
replayer.durable = true

